{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e4cd88-eb56-47b8-bb81-37d219057e65",
   "metadata": {},
   "source": [
    "# Preprocess for casimedicos CONLL files\n",
    "\n",
    "This notebooks preprocess the CONLL files from `casimedicos` in order to normalize them to a more common CONLL format, where each sentence is grouped together (no blank lines in-between words of the same sentence, only between sentences), and each paragraph (defined by the reset of the index counter) is spaced by two blank lines. It also tokenizes (separate) common puntuation symbols: `'\"!?(),;:[]><=.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecff11-a181-4219-be97-17e994a5d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a7b96-27ea-4caa-9f0c-4380c624f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = Path('../data/casimedicos/')\n",
    "SPLITS = ['train', 'dev', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda452aa-90e0-4c7f-90d0-483fdb44e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    if line.strip() == '':\n",
    "        # The line is empty\n",
    "        return {'idx': None, 'label': None, 'tokens': []}\n",
    "\n",
    "    idx, token, _, _, label = line.strip().split()\n",
    "    idx = int(idx)\n",
    "    tokens = []\n",
    "\n",
    "    if re.match(r\"[0-9]+\\.$\", token):\n",
    "        # The token is a number, but ends with a dot, which might be the end of a sentence\n",
    "        tokens = [token[:-1], \".\"]\n",
    "    elif token.replace('.', '', 1).isdigit():\n",
    "        # The token represents a whole number, might be a float, we should not replace it\n",
    "        tokens = [token]\n",
    "    else:\n",
    "        # The token is neither a float or a number that ends with a dot, we can split it by puntuations\n",
    "        tokens = [tk for tk in re.split('([,?:;~!#$%^&*()\\[\\]\"\\'><=.-])', token) if tk != '']\n",
    "\n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'label': label,\n",
    "        'tokens': tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81485c67-3684-426a-8a7d-2b50903aa15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in SPLITS:\n",
    "    with open(DIR / f'{split}.conll', 'rt') as fhi, open(DIR / f'{split}_revisited.conll', 'wt') as fho:\n",
    "        last_line = None\n",
    "        paragraph_lines = 0\n",
    "        for line in tqdm(fhi):\n",
    "            new_line = process_line(line)\n",
    "            if len(new_line['tokens']) == 0:\n",
    "                continue  # Omit empty lines            \n",
    "    \n",
    "            if last_line is None:\n",
    "                last_line = new_line\n",
    "                continue  # We cannot do anything without at least 2 lines\n",
    "    \n",
    "            for idx, token in enumerate(last_line['tokens'], start=paragraph_lines):\n",
    "                print(f\"{idx}\\t{token}\\t_\\t_\\t{last_line['label']}\", file=fho)\n",
    "    \n",
    "            paragraph_lines += len(last_line['tokens'])\n",
    "    \n",
    "            if new_line['idx'] < last_line['idx']:\n",
    "                # We have a new paragraph\n",
    "                paragraph_lines = 0\n",
    "                print('', end='\\n\\n', file=fho)  # Print two blank lines for the end of a paragraph\n",
    "            elif last_line['tokens'][-1] == '.' and last_line['label'] != 'O' and last_line['label'] != new_line['label']:\n",
    "                # We have a dot signifiying the end of a sentence and the previous line label\n",
    "                # is not an 'O' label and is not the same as the new line label (e.g. two 'I-*' labels)\n",
    "                print('', file=fho) # Print an empty line for the end of a sentence\n",
    "            last_line = new_line\n",
    "    \n",
    "        if last_line:\n",
    "            for idx, token in enumerate(last_line['tokens'], start=paragraph_lines):\n",
    "                print(f\"{idx}\\t{token}\\t_\\t_\\t{last_line['label']}\", file=fho)\n",
    "            print('', file=fho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
